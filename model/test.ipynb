{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from dalle_pytorch import DiscreteVAE, DALLE\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from dataloader import CustomImageDataLoader\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_chans_io, dec_chans_io: [(3, 64), (64, 64), (64, 64)] [(64, 64), (64, 64), (64, 64)]\n",
      "dec_layers: 512 64\n",
      "normalization: ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n"
     ]
    }
   ],
   "source": [
    "vae = DiscreteVAE(\n",
    "    image_size = 512,\n",
    "    num_layers = 3,           # number of downsamples - ex. 256 / (2 ** 3) = (32 x 32 feature map)\n",
    "    num_tokens = 8192,        # number of visual tokens. in the paper, they used 8192, but could be smaller for downsized projects\n",
    "    codebook_dim = 512,       # codebook dimension\n",
    "    hidden_dim = 64,          # hidden dimension\n",
    "    num_resnet_blocks = 2,    # number of resnet blocks\n",
    "    temperature = 0.9,        # gumbel softmax temperature, the lower this is, the harder the discretization\n",
    "    straight_through = False, # straight-through for gumbel softmax. unclear if it is better one way or the other\n",
    ").to('cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8192, 64, 64])\n",
      "temp: 0.9\n",
      "sampled torch.Size([1, 512, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "img = torch.randn(1, 3, 512, 512).to('cuda')\n",
    "\n",
    "f = vae.forward(img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           3,136\n",
      "              ReLU-2         [-1, 64, 256, 256]               0\n",
      "            Conv2d-3         [-1, 64, 128, 128]          65,600\n",
      "              ReLU-4         [-1, 64, 128, 128]               0\n",
      "            Conv2d-5           [-1, 64, 64, 64]          65,600\n",
      "              ReLU-6           [-1, 64, 64, 64]               0\n",
      "            Conv2d-7           [-1, 64, 64, 64]          36,928\n",
      "              ReLU-8           [-1, 64, 64, 64]               0\n",
      "            Conv2d-9           [-1, 64, 64, 64]          36,928\n",
      "             ReLU-10           [-1, 64, 64, 64]               0\n",
      "           Conv2d-11           [-1, 64, 64, 64]           4,160\n",
      "         ResBlock-12           [-1, 64, 64, 64]               0\n",
      "           Conv2d-13           [-1, 64, 64, 64]          36,928\n",
      "             ReLU-14           [-1, 64, 64, 64]               0\n",
      "           Conv2d-15           [-1, 64, 64, 64]          36,928\n",
      "             ReLU-16           [-1, 64, 64, 64]               0\n",
      "           Conv2d-17           [-1, 64, 64, 64]           4,160\n",
      "         ResBlock-18           [-1, 64, 64, 64]               0\n",
      "           Conv2d-19         [-1, 8192, 64, 64]         532,480\n",
      "           Conv2d-20           [-1, 64, 64, 64]          32,832\n",
      "           Conv2d-21           [-1, 64, 64, 64]          36,928\n",
      "             ReLU-22           [-1, 64, 64, 64]               0\n",
      "           Conv2d-23           [-1, 64, 64, 64]          36,928\n",
      "             ReLU-24           [-1, 64, 64, 64]               0\n",
      "           Conv2d-25           [-1, 64, 64, 64]           4,160\n",
      "         ResBlock-26           [-1, 64, 64, 64]               0\n",
      "           Conv2d-27           [-1, 64, 64, 64]          36,928\n",
      "             ReLU-28           [-1, 64, 64, 64]               0\n",
      "           Conv2d-29           [-1, 64, 64, 64]          36,928\n",
      "             ReLU-30           [-1, 64, 64, 64]               0\n",
      "           Conv2d-31           [-1, 64, 64, 64]           4,160\n",
      "         ResBlock-32           [-1, 64, 64, 64]               0\n",
      "  ConvTranspose2d-33         [-1, 64, 128, 128]          65,600\n",
      "             ReLU-34         [-1, 64, 128, 128]               0\n",
      "  ConvTranspose2d-35         [-1, 64, 256, 256]          65,600\n",
      "             ReLU-36         [-1, 64, 256, 256]               0\n",
      "  ConvTranspose2d-37         [-1, 64, 512, 512]          65,600\n",
      "             ReLU-38         [-1, 64, 512, 512]               0\n",
      "           Conv2d-39          [-1, 3, 512, 512]             195\n",
      "================================================================\n",
      "Total params: 1,208,707\n",
      "Trainable params: 1,208,707\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 732.00\n",
      "Params size (MB): 4.61\n",
      "Estimated Total Size (MB): 739.61\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(vae, (3, 512, 512))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "path_images = 'F:/DATASETS/original/images'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "dataset = CustomImageDataLoader(path_dataset=path_images)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=None,\n",
    "    pin_memory=False,\n",
    " )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def padd_image(image):\n",
    "\n",
    "    batch_size = image.shape[0]\n",
    "\n",
    "    image = image.resize(batch_size, 3, 240, 360)\n",
    "\n",
    "    source = (240, 360)\n",
    "    objective = (512, 512)\n",
    "\n",
    "    num_blank_rows = objective[1] - source[1]\n",
    "    num_pad_col = objective[0] - source[0]\n",
    "\n",
    "    top = torch.zeros(batch_size, 3, objective[0], num_blank_rows//2)\n",
    "    bot = torch.zeros(batch_size, 3, objective[0], num_blank_rows//2)\n",
    "\n",
    "    left = torch.zeros(batch_size, 3, num_pad_col//2, source[1])\n",
    "    right = torch.zeros(batch_size, 3, num_pad_col//2, source[1])\n",
    "\n",
    "    # print(left.shape, image.shape)\n",
    "\n",
    "    concated = torch.concat((left, image), dim=2)\n",
    "    concated = torch.concat((concated, right), dim=2)\n",
    "\n",
    "    concated = torch.concat((top, concated), dim=3)\n",
    "    concated = torch.concat((concated, bot), dim=3)\n",
    "\n",
    "\n",
    "    return concated"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('test')\n",
    "\n",
    "# %time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 8.00 GiB total capacity; 6.70 GiB already allocated; 0 bytes free; 6.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[1;32m<timed exec>:6\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\dalle_pytorch\\dalle_pytorch.py:230\u001B[0m, in \u001B[0;36mDiscreteVAE.forward\u001B[1;34m(self, img, return_loss, return_recons, return_logits, temp)\u001B[0m\n\u001B[0;32m    227\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m logits \u001B[38;5;66;03m# return logits for getting hard image indices for DALL-E training\u001B[39;00m\n\u001B[0;32m    229\u001B[0m temp \u001B[38;5;241m=\u001B[39m default(temp, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtemperature)\n\u001B[1;32m--> 230\u001B[0m soft_one_hot \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgumbel_softmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtau\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtemp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhard\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstraight_through\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m sampled \u001B[38;5;241m=\u001B[39m einsum(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb n h w, n d -> b d h w\u001B[39m\u001B[38;5;124m'\u001B[39m, soft_one_hot, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcodebook\u001B[38;5;241m.\u001B[39mweight)\n\u001B[0;32m    232\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(sampled)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:1872\u001B[0m, in \u001B[0;36mgumbel_softmax\u001B[1;34m(logits, tau, hard, eps, dim)\u001B[0m\n\u001B[0;32m   1867\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`eps` parameter is deprecated and has no effect.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1869\u001B[0m gumbels \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   1870\u001B[0m     \u001B[38;5;241m-\u001B[39mtorch\u001B[38;5;241m.\u001B[39mempty_like(logits, memory_format\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlegacy_contiguous_format)\u001B[38;5;241m.\u001B[39mexponential_()\u001B[38;5;241m.\u001B[39mlog()\n\u001B[0;32m   1871\u001B[0m )  \u001B[38;5;66;03m# ~Gumbel(0,1)\u001B[39;00m\n\u001B[1;32m-> 1872\u001B[0m gumbels \u001B[38;5;241m=\u001B[39m \u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mgumbels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtau\u001B[49m  \u001B[38;5;66;03m# ~Gumbel(logits,tau)\u001B[39;00m\n\u001B[0;32m   1873\u001B[0m y_soft \u001B[38;5;241m=\u001B[39m gumbels\u001B[38;5;241m.\u001B[39msoftmax(dim)\n\u001B[0;32m   1875\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m hard:\n\u001B[0;32m   1876\u001B[0m     \u001B[38;5;66;03m# Straight through.\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 8.00 GiB total capacity; 6.70 GiB already allocated; 0 bytes free; 6.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "i = 0\n",
    "\n",
    "for batch in dataloader:\n",
    "    padded = (padd_image(batch['image'].resize(4, 3, 360, 240))).to('cuda')\n",
    "\n",
    "    loss = vae(padded, return_loss=True)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    if i == 100:\n",
    "        break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}