{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "import torch\n",
    "from dalle_pytorch import DiscreteVAE, DALLE\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "vae = DiscreteVAE(\n",
    "    image_size = 256,\n",
    "    num_layers = 3,           # number of downsamples - ex. 256 / (2 ** 3) = (32 x 32 feature map)\n",
    "    num_tokens = 8192,        # number of visual tokens. in the paper, they used 8192, but could be smaller for downsized projects\n",
    "    codebook_dim = 512,       # codebook dimension\n",
    "    hidden_dim = 64,          # hidden dimension\n",
    "    num_resnet_blocks = 2,    # number of resnet blocks\n",
    "    temperature = 0.9,        # gumbel softmax temperature, the lower this is, the harder the discretization\n",
    "    straight_through = False, # straight-through for gumbel softmax. unclear if it is better one way or the other\n",
    ").to('cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 180, 120]           3,136\n",
      "              ReLU-2         [-1, 64, 180, 120]               0\n",
      "            Conv2d-3           [-1, 64, 90, 60]          65,600\n",
      "              ReLU-4           [-1, 64, 90, 60]               0\n",
      "            Conv2d-5           [-1, 64, 45, 30]          16,448\n",
      "              ReLU-6           [-1, 64, 45, 30]               0\n",
      "            Conv2d-7           [-1, 64, 45, 30]          36,928\n",
      "              ReLU-8           [-1, 64, 45, 30]               0\n",
      "            Conv2d-9           [-1, 64, 45, 30]          36,928\n",
      "             ReLU-10           [-1, 64, 45, 30]               0\n",
      "           Conv2d-11           [-1, 64, 45, 30]          36,928\n",
      "             ReLU-12           [-1, 64, 45, 30]               0\n",
      "           Conv2d-13           [-1, 64, 45, 30]          36,928\n",
      "             ReLU-14           [-1, 64, 45, 30]               0\n",
      "           Conv2d-15           [-1, 64, 45, 30]          36,928\n",
      "             ReLU-16           [-1, 64, 45, 30]               0\n",
      "           Conv2d-17          [-1, 128, 22, 15]         131,200\n",
      "           Conv2d-18         [-1, 4096, 22, 15]         528,384\n",
      "================================================================\n",
      "Total params: 929,408\n",
      "Trainable params: 929,408\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.99\n",
      "Forward/backward pass size (MB): 44.91\n",
      "Params size (MB): 3.55\n",
      "Estimated Total Size (MB): 49.45\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ImageCompressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Image compressor in order to retain local information for the GAN\n",
    "\n",
    "    Input: Image (240x360)\n",
    "    Output: Vector (1x512)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ImageCompressor, self).__init__()\n",
    "\n",
    "        num_blocks = 5\n",
    "        num_tokens = 4096\n",
    "\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "\n",
    "        for _ in range(num_blocks):\n",
    "            layers.append(nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        layers.append(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=(1, 1)))\n",
    "        layers.append(nn.Conv2d(128, num_tokens, kernel_size=1))\n",
    "\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, image):\n",
    "        # image = torchvision.transforms.functional.to_tensor(image.numpy())\n",
    "        return self.layers(image)\n",
    "\n",
    "c = ImageCompressor().to('cuda')\n",
    "summary(c, (3, 360, 240))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           3,136\n",
      "              ReLU-2         [-1, 64, 128, 128]               0\n",
      "            Conv2d-3           [-1, 64, 64, 64]          65,600\n",
      "              ReLU-4           [-1, 64, 64, 64]               0\n",
      "            Conv2d-5           [-1, 64, 32, 32]          65,600\n",
      "              ReLU-6           [-1, 64, 32, 32]               0\n",
      "            Conv2d-7           [-1, 64, 32, 32]          36,928\n",
      "              ReLU-8           [-1, 64, 32, 32]               0\n",
      "            Conv2d-9           [-1, 64, 32, 32]          36,928\n",
      "             ReLU-10           [-1, 64, 32, 32]               0\n",
      "           Conv2d-11           [-1, 64, 32, 32]           4,160\n",
      "         ResBlock-12           [-1, 64, 32, 32]               0\n",
      "           Conv2d-13           [-1, 64, 32, 32]          36,928\n",
      "             ReLU-14           [-1, 64, 32, 32]               0\n",
      "           Conv2d-15           [-1, 64, 32, 32]          36,928\n",
      "             ReLU-16           [-1, 64, 32, 32]               0\n",
      "           Conv2d-17           [-1, 64, 32, 32]           4,160\n",
      "         ResBlock-18           [-1, 64, 32, 32]               0\n",
      "           Conv2d-19         [-1, 8192, 32, 32]         532,480\n",
      "================================================================\n",
      "Total params: 822,848\n",
      "Trainable params: 822,848\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 91.00\n",
      "Params size (MB): 3.14\n",
      "Estimated Total Size (MB): 94.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(vae.encoder, (3, 256, 256))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           3,136\n",
      "              ReLU-2         [-1, 64, 128, 128]               0\n",
      "            Conv2d-3           [-1, 64, 64, 64]          65,600\n",
      "              ReLU-4           [-1, 64, 64, 64]               0\n",
      "            Conv2d-5           [-1, 64, 32, 32]          65,600\n",
      "              ReLU-6           [-1, 64, 32, 32]               0\n",
      "            Conv2d-7           [-1, 64, 32, 32]          36,928\n",
      "              ReLU-8           [-1, 64, 32, 32]               0\n",
      "            Conv2d-9           [-1, 64, 32, 32]          36,928\n",
      "             ReLU-10           [-1, 64, 32, 32]               0\n",
      "           Conv2d-11           [-1, 64, 32, 32]           4,160\n",
      "         ResBlock-12           [-1, 64, 32, 32]               0\n",
      "           Conv2d-13           [-1, 64, 32, 32]          36,928\n",
      "             ReLU-14           [-1, 64, 32, 32]               0\n",
      "           Conv2d-15           [-1, 64, 32, 32]          36,928\n",
      "             ReLU-16           [-1, 64, 32, 32]               0\n",
      "           Conv2d-17           [-1, 64, 32, 32]           4,160\n",
      "         ResBlock-18           [-1, 64, 32, 32]               0\n",
      "           Conv2d-19         [-1, 8192, 32, 32]         532,480\n",
      "           Conv2d-20           [-1, 64, 32, 32]          32,832\n",
      "           Conv2d-21           [-1, 64, 32, 32]          36,928\n",
      "             ReLU-22           [-1, 64, 32, 32]               0\n",
      "           Conv2d-23           [-1, 64, 32, 32]          36,928\n",
      "             ReLU-24           [-1, 64, 32, 32]               0\n",
      "           Conv2d-25           [-1, 64, 32, 32]           4,160\n",
      "         ResBlock-26           [-1, 64, 32, 32]               0\n",
      "           Conv2d-27           [-1, 64, 32, 32]          36,928\n",
      "             ReLU-28           [-1, 64, 32, 32]               0\n",
      "           Conv2d-29           [-1, 64, 32, 32]          36,928\n",
      "             ReLU-30           [-1, 64, 32, 32]               0\n",
      "           Conv2d-31           [-1, 64, 32, 32]           4,160\n",
      "         ResBlock-32           [-1, 64, 32, 32]               0\n",
      "  ConvTranspose2d-33           [-1, 64, 64, 64]          65,600\n",
      "             ReLU-34           [-1, 64, 64, 64]               0\n",
      "  ConvTranspose2d-35         [-1, 64, 128, 128]          65,600\n",
      "             ReLU-36         [-1, 64, 128, 128]               0\n",
      "  ConvTranspose2d-37         [-1, 64, 256, 256]          65,600\n",
      "             ReLU-38         [-1, 64, 256, 256]               0\n",
      "           Conv2d-39          [-1, 3, 256, 256]             195\n",
      "================================================================\n",
      "Total params: 1,208,707\n",
      "Trainable params: 1,208,707\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 183.00\n",
      "Params size (MB): 4.61\n",
      "Estimated Total Size (MB): 188.36\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(vae, (3, 256, 256))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 3, 256, 256])"
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = torch.randn(1, 3, 256, 256).to('cuda')\n",
    "encoded = vae.forward(images)\n",
    "# vae.decode(encoded).shape\n",
    "encoded.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "dalle = DALLE(\n",
    "    dim = 1024,\n",
    "    vae = vae,                  # automatically infer (1) image sequence length and (2) number of image tokens\n",
    "    num_text_tokens = 10000,    # vocab size for text\n",
    "    text_seq_len = 256,         # text sequence length\n",
    "    depth = 12,                 # should aim to be 64\n",
    "    heads = 16,                 # attention heads\n",
    "    dim_head = 64,              # attention head dimension\n",
    "    attn_dropout = 0.1,         # attention dropout\n",
    "    ff_dropout = 0.1            # feedforward dropout\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}